{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423a39f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41bf10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_ds = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"raft_train.jsonl\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "# Optionally: split into train/test\n",
    "splits = raw_ds.train_test_split(test_size=0.1)\n",
    "train_ds = splits[\"train\"]\n",
    "eval_ds  = splits[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28653a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>oracle_context</th>\n",
       "      <th>cot_answer</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seed_task_0</td>\n",
       "      <td>general</td>\n",
       "      <td>How many learners participated in the study?</td>\n",
       "      <td>{'sentences': [['Table 4: Local Average Treatm...</td>\n",
       "      <td>The value of non-traditional credentials in th...</td>\n",
       "      <td>assistant: To determine how many learners part...</td>\n",
       "      <td>&lt;DOCUMENT&gt;Table 4: Local Average Treatment Eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seed_task_1</td>\n",
       "      <td>general</td>\n",
       "      <td>What platforms were used to deliver the courses?</td>\n",
       "      <td>{'sentences': [['The value of non-traditional ...</td>\n",
       "      <td>The value of non-traditional credentials in th...</td>\n",
       "      <td>assistant: Step 1: Identify relevant informati...</td>\n",
       "      <td>&lt;DOCUMENT&gt;The value of non-traditional credent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seed_task_2</td>\n",
       "      <td>general</td>\n",
       "      <td>What was the main finding regarding credential...</td>\n",
       "      <td>{'sentences': [['The value of non-traditional ...</td>\n",
       "      <td>The value of non-traditional credentials in th...</td>\n",
       "      <td>assistant: ### Step-by-Step Reasoning:\\n1. **I...</td>\n",
       "      <td>&lt;DOCUMENT&gt;The value of non-traditional credent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seed_task_3</td>\n",
       "      <td>general</td>\n",
       "      <td>Did the intervention have a significant impact...</td>\n",
       "      <td>{'sentences': [['The value of non-traditional ...</td>\n",
       "      <td>The value of non-traditional credentials in th...</td>\n",
       "      <td>assistant: Step-by-step reasoning:\\n\\n1. The q...</td>\n",
       "      <td>&lt;DOCUMENT&gt;The value of non-traditional credent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seed_task_4</td>\n",
       "      <td>general</td>\n",
       "      <td>How did the effect vary among different groups...</td>\n",
       "      <td>{'sentences': [['13 p.p.) and 36 p.p. (S.E. 12...</td>\n",
       "      <td>The value of non-traditional credentials in th...</td>\n",
       "      <td>assistant: Step-by-step reasoning:\\n\\n1. The s...</td>\n",
       "      <td>&lt;DOCUMENT&gt;13 p.p.) and 36 p.p. (S.E. 12 p.p.) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     type                                           question  \\\n",
       "0  seed_task_0  general       How many learners participated in the study?   \n",
       "1  seed_task_1  general   What platforms were used to deliver the courses?   \n",
       "2  seed_task_2  general  What was the main finding regarding credential...   \n",
       "3  seed_task_3  general  Did the intervention have a significant impact...   \n",
       "4  seed_task_4  general  How did the effect vary among different groups...   \n",
       "\n",
       "                                             context  \\\n",
       "0  {'sentences': [['Table 4: Local Average Treatm...   \n",
       "1  {'sentences': [['The value of non-traditional ...   \n",
       "2  {'sentences': [['The value of non-traditional ...   \n",
       "3  {'sentences': [['The value of non-traditional ...   \n",
       "4  {'sentences': [['13 p.p.) and 36 p.p. (S.E. 12...   \n",
       "\n",
       "                                      oracle_context  \\\n",
       "0  The value of non-traditional credentials in th...   \n",
       "1  The value of non-traditional credentials in th...   \n",
       "2  The value of non-traditional credentials in th...   \n",
       "3  The value of non-traditional credentials in th...   \n",
       "4  The value of non-traditional credentials in th...   \n",
       "\n",
       "                                          cot_answer  \\\n",
       "0  assistant: To determine how many learners part...   \n",
       "1  assistant: Step 1: Identify relevant informati...   \n",
       "2  assistant: ### Step-by-Step Reasoning:\\n1. **I...   \n",
       "3  assistant: Step-by-step reasoning:\\n\\n1. The q...   \n",
       "4  assistant: Step-by-step reasoning:\\n\\n1. The s...   \n",
       "\n",
       "                                         instruction  \n",
       "0  <DOCUMENT>Table 4: Local Average Treatment Eff...  \n",
       "1  <DOCUMENT>The value of non-traditional credent...  \n",
       "2  <DOCUMENT>The value of non-traditional credent...  \n",
       "3  <DOCUMENT>The value of non-traditional credent...  \n",
       "4  <DOCUMENT>13 p.p.) and 36 p.p. (S.E. 12 p.p.) ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(raw_ds)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a130e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'type', 'question', 'context', 'oracle_context', 'cot_answer', 'instruction'],\n",
       "     num_rows: 296\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'type', 'question', 'context', 'oracle_context', 'cot_answer', 'instruction'],\n",
       "     num_rows: 33\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8159bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-20 06:21:45 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-20 06:21:45 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 22.184 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f44172c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.7 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 2025,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f80828e",
   "metadata": {},
   "source": [
    "## Formatting the prompts\n",
    "We need to put everything together into a single 'text' field for the LLM to be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81f90657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab2864a83f7441f8d07cf1a7986b0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2653e1ac672743d28fa0c0fe85bf1db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Define a formatter that injects the retrieved context:\"\"\"\n",
    "    \n",
    "    texts = []\n",
    "    for ctx, oracle, instr, ans in zip(\n",
    "        examples[\"context\"],\n",
    "        examples[\"oracle_context\"],\n",
    "        examples[\"instruction\"],\n",
    "        examples[\"cot_answer\"]\n",
    "    ):\n",
    "        # You can choose to use `oracle_context` (gold) vs. `context` (retrieved)\n",
    "        # Here we show both, but you could just use `context`.\n",
    "        prompt = (\n",
    "            \"### Retrieved Passages:\\n\"\n",
    "            f\"{ctx}\\n\\n\"\n",
    "            \"### (Oracle Passages):\\n\"\n",
    "            f\"{oracle}\\n\\n\"\n",
    "            \"### Question:\\n\"\n",
    "            f\"{instr}\\n\\n\"\n",
    "            \"### Answer:\\n\"\n",
    "        )\n",
    "        # Append the gold answer plus EOS\n",
    "        texts.append(prompt + ans + tokenizer.eos_token)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# then:\n",
    "train_ds = train_ds.map(formatting_prompts_func, batched=True)\n",
    "eval_ds = eval_ds.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5495f1",
   "metadata": {},
   "source": [
    "Here's an example of the text we are going to train our LLM on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17387ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Retrieved Passages:\n",
       "{'sentences': [['• If the time difference is greater than or equal to 0 months, it implies a recent career devel-\\nopment, leading to further analysis:\\n– If “Intern” is found in the current job title, the outcome is classified as a new internship.\\n– If the employer of the current job differs from the employer of the previous job, it\\nindicates a change in job roles, leading to the classification of new job.\\n', 'The value of non-traditional credentials in the labor market*\\nSusan Athey & Emil Palikot\\nMay 2, 2024\\nAbstract\\nThis study investigates the labor market value of credentials obtained from Massive Open On-\\nline Courses (MOOCs) and shared on business networking platforms. We conducted a random-\\nized experiment involving more than 800,000 learners, primarily from developing countries and\\nwithout college degrees, who completed technology or business-related courses on the Coursera\\nplatform between September 2022 and March 2023. The intervention targeted learners who had\\nrecently completed their courses, encouraging them to share their credentials and simplifying the\\nsharing process. One year after the intervention, we collected data from LinkedIn profiles of ap-\\nproximately 40,000 experimental subjects. We find that the intervention leads to an increase of 17\\npercentage points for credential sharing. Further, learners in the treatment group were 6% more\\nlikely to report new employment within a year, with an 8% increase in jobs related to their certifi-\\ncates. This effect was more pronounced among LinkedIn users with lower baseline employability.\\nAcross the entire sample, the treated group received a higher number of certificate views, indicat-\\ning an increased interest in their profiles. These results suggest that facilitating credential sharing\\nand reminding learners of the value of skill signaling can yield significant gains. When the ex-\\nperiment is viewed as an encouragement design for credential sharing, we can estimate the local\\naverage treatment effect (LATE) of credential sharing (that is, the impact of credential sharing on\\nthe workers induced to share by the intervention) for the outcome of getting a job. The LATE esti-\\nmates are imprecise but large in magnitude; they suggest that credential sharing more than doubles\\nthe baseline probability of getting a new job in scope for the credential.\\n*We thank Eric Karsten and his team in Coursera for collaborating on this project. ', '(2018). Can mooc programs improve student employment\\nprospects? Available at SSRN 3260695.\\nHansen, J. D. and Reich, J. (2015). Democratizing education? examining access and usage patterns in\\nmassive open online courses. Science, 350(6265):1245–1248.\\nHussey, A. (2012). Human capital augmentation versus the signaling value of mba education. Eco-\\nnomics of Education Review, 31(4):442–451.\\n19']], 'title': [['placeholder_title', 'placeholder_title', 'placeholder_title']]}\n",
       "\n",
       "### (Oracle Passages):\n",
       "• If the time difference is greater than or equal to 0 months, it implies a recent career devel-\n",
       "opment, leading to further analysis:\n",
       "– If “Intern” is found in the current job title, the outcome is classified as a new internship.\n",
       "– If the employer of the current job differs from the employer of the previous job, it\n",
       "indicates a change in job roles, leading to the classification of new job.\n",
       "\n",
       "\n",
       "### Question:\n",
       "<DOCUMENT>• If the time difference is greater than or equal to 0 months, it implies a recent career devel-\n",
       "opment, leading to further analysis:\n",
       "– If “Intern” is found in the current job title, the outcome is classified as a new internship.\n",
       "– If the employer of the current job differs from the employer of the previous job, it\n",
       "indicates a change in job roles, leading to the classification of new job.\n",
       "</DOCUMENT>\n",
       "<DOCUMENT>The value of non-traditional credentials in the labor market*\n",
       "Susan Athey & Emil Palikot\n",
       "May 2, 2024\n",
       "Abstract\n",
       "This study investigates the labor market value of credentials obtained from Massive Open On-\n",
       "line Courses (MOOCs) and shared on business networking platforms. We conducted a random-\n",
       "ized experiment involving more than 800,000 learners, primarily from developing countries and\n",
       "without college degrees, who completed technology or business-related courses on the Coursera\n",
       "platform between September 2022 and March 2023. The intervention targeted learners who had\n",
       "recently completed their courses, encouraging them to share their credentials and simplifying the\n",
       "sharing process. One year after the intervention, we collected data from LinkedIn profiles of ap-\n",
       "proximately 40,000 experimental subjects. We find that the intervention leads to an increase of 17\n",
       "percentage points for credential sharing. Further, learners in the treatment group were 6% more\n",
       "likely to report new employment within a year, with an 8% increase in jobs related to their certifi-\n",
       "cates. This effect was more pronounced among LinkedIn users with lower baseline employability.\n",
       "Across the entire sample, the treated group received a higher number of certificate views, indicat-\n",
       "ing an increased interest in their profiles. These results suggest that facilitating credential sharing\n",
       "and reminding learners of the value of skill signaling can yield significant gains. When the ex-\n",
       "periment is viewed as an encouragement design for credential sharing, we can estimate the local\n",
       "average treatment effect (LATE) of credential sharing (that is, the impact of credential sharing on\n",
       "the workers induced to share by the intervention) for the outcome of getting a job. The LATE esti-\n",
       "mates are imprecise but large in magnitude; they suggest that credential sharing more than doubles\n",
       "the baseline probability of getting a new job in scope for the credential.\n",
       "*We thank Eric Karsten and his team in Coursera for collaborating on this project. </DOCUMENT>\n",
       "<DOCUMENT>(2018). Can mooc programs improve student employment\n",
       "prospects? Available at SSRN 3260695.\n",
       "Hansen, J. D. and Reich, J. (2015). Democratizing education? examining access and usage patterns in\n",
       "massive open online courses. Science, 350(6265):1245–1248.\n",
       "Hussey, A. (2012). Human capital augmentation versus the signaling value of mba education. Eco-\n",
       "nomics of Education Review, 31(4):442–451.\n",
       "19</DOCUMENT>\n",
       "What does a time difference greater than or equal to 0 months indicate?\n",
       "\n",
       "### Answer:\n",
       "assistant: Step-by-step reasoning:\n",
       "1. The question asks about what a time difference greater than or equal to 0 months indicates.\n",
       "2. According to the given context, this condition (time difference ≥ 0 months) implies a recent career development. ##begin_quote##If the time difference is greater than or equal to 0 months, it implies a recent career development, leading to further analysis:##end_quote##\n",
       "3. The context then provides two specific scenarios for such cases:\n",
       "   - If \"Intern\" is found in the current job title, it's classified as a new internship.\n",
       "   - If the employer of the current job differs from that of the previous job, it indicates a change in job roles and is thus classified as a new job.\n",
       "\n",
       "Final answer: <ANSWER>: A time difference greater than or equal to 0 months indicates recent career development, which could be either a new internship (if \"Intern\" is in the current job title) or a new job due to a change in employer.<|eot_id|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(pd.DataFrame(train_ds).head()['text'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b69c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94014548c7b64940bd089b03852f7536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34854d403b64c69817e48ffea8e5f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llama32_1bn_raft\", #This will also be used as your huggingfacehub model id name\n",
    "    report_to=\"wandb\", #Leave this to be blank if you don't want to use wandb\n",
    "    run_name=\"RAFT_SFT_Take6\",\n",
    "    eval_steps=5,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_train_batch_size=1,    # small batches if quantized\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    # max_steps=60,                    # or set num_train_epochs\n",
    "    save_strategy=\"no\",\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    seed=42,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = eval_ds, \n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636c9cf",
   "metadata": {},
   "source": [
    "Current memory statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e70fcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A10G. Max memory = 22.184 GB.\n",
      "1.457 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9280646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 296 | Num Epochs = 5 | Total steps = 185\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 11,272,192/1,000,000,000 (1.13% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtituslhy\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/ideal-palm-tree/notebooks/wandb/run-20250520_062235-dfyyu2be</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tituslhy/huggingface/runs/dfyyu2be' target=\"_blank\">RAFT_SFT_Take6</a></strong> to <a href='https://wandb.ai/tituslhy/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tituslhy/huggingface' target=\"_blank\">https://wandb.ai/tituslhy/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tituslhy/huggingface/runs/dfyyu2be' target=\"_blank\">https://wandb.ai/tituslhy/huggingface/runs/dfyyu2be</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='185' max='185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [185/185 10:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.596200</td>\n",
       "      <td>1.565817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.570800</td>\n",
       "      <td>1.551308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.584700</td>\n",
       "      <td>1.529841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.604600</td>\n",
       "      <td>1.506040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.484400</td>\n",
       "      <td>1.481573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.491900</td>\n",
       "      <td>1.458403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.456100</td>\n",
       "      <td>1.435972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>1.414349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.375600</td>\n",
       "      <td>1.392648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.454300</td>\n",
       "      <td>1.372671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.387700</td>\n",
       "      <td>1.352372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.372400</td>\n",
       "      <td>1.330699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.327000</td>\n",
       "      <td>1.311188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.368400</td>\n",
       "      <td>1.292958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.302100</td>\n",
       "      <td>1.275678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.247500</td>\n",
       "      <td>1.260039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.296700</td>\n",
       "      <td>1.246341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.242300</td>\n",
       "      <td>1.233546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.293000</td>\n",
       "      <td>1.221592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.247600</td>\n",
       "      <td>1.210945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.221000</td>\n",
       "      <td>1.201311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.281700</td>\n",
       "      <td>1.192325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.253000</td>\n",
       "      <td>1.184831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.159200</td>\n",
       "      <td>1.178321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.214600</td>\n",
       "      <td>1.173163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.239000</td>\n",
       "      <td>1.168511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.157100</td>\n",
       "      <td>1.164658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.162700</td>\n",
       "      <td>1.161669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.221800</td>\n",
       "      <td>1.159015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.153500</td>\n",
       "      <td>1.157004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.253100</td>\n",
       "      <td>1.155509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.134700</td>\n",
       "      <td>1.154566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.150300</td>\n",
       "      <td>1.153851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.157800</td>\n",
       "      <td>1.153360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.184000</td>\n",
       "      <td>1.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.186400</td>\n",
       "      <td>1.152263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.189300</td>\n",
       "      <td>1.152938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370ba88",
   "metadata": {},
   "source": [
    "Used memory statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76dbfe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617.3861 seconds used for training.\n",
      "10.29 minutes used for training.\n",
      "Peak reserved memory = 2.156 GB.\n",
      "Peak reserved memory for training = 0.699 GB.\n",
      "Peak reserved memory % of max memory = 9.719 %.\n",
      "Peak reserved memory for training % of max memory = 3.151 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "123e0ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\n",
      "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
      "To force `safe_serialization`, set it to `None` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 2.33 out of 15.42 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 67.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving llama32_1bn_raft_merged/pytorch_model.bin...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\n",
    "    save_directory = \"llama32_1bn_raft_merged\",     # Local path to store merged model\n",
    "    tokenizer = tokenizer,\n",
    "    save_method = \"merged_16bit\",        # Can also use \"merged_4bit\" or \"merged_8bit\" if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1634acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4db4cbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You are pushing to hub, but you passed your HF username = tituslhy.\n",
      "We shall truncate tituslhy/llama32_1bn_raft_non_traditional_credentials to llama32_1bn_raft_non_traditional_credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 2.17 out of 15.42 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 101.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b267f59c4a7248fdafc2ba2e2d60d367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Unsloth: Saving llama32_1bn_raft_non_traditional_credentials/pytorch_model.bin...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d73ff5a466d4a99aeb9747409ba0fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d390f74724b1497193228773971f9f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/tituslhy/llama32_1bn_raft_non_traditional_credentials\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub_merged(\n",
    "    repo_id=\"tituslhy/llama32_1bn_raft_non_traditional_credentials\",\n",
    "    tokenizer=tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=os.environ[\"HUGGINGFACE_ACCESS_TOKEN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190574e4",
   "metadata": {},
   "source": [
    "You don't have to run this cell if your llama.cpp gguf execution (`model.push_to_hub_gguf`) works! \n",
    "> I had to because it could not find where the llama-quantize binary was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "104a1b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook sees: ['.github', 'CODEOWNERS', 'pyproject.toml', 'README.md', 'gguf-py', 'ggml', '.clang-tidy', '.pre-commit-config.yaml', 'examples', 'tests', 'convert_llama_ggml_to_gguf.py', 'cmake', '.gitignore', 'CMakeLists.txt', 'build-xcframework.sh', 'scripts', 'Makefile', 'pocs', 'pyrightconfig.json', 'poetry.lock', 'convert_hf_to_gguf_update.py', 'src', 'docs', 'convert_hf_to_gguf.py', 'mypy.ini', 'llama-quantize', 'CONTRIBUTING.md', 'models', '.git', '.dockerignore', 'AUTHORS', 'requirements.txt', 'licenses', '.clang-format', 'flake.nix', 'prompts', 'tools', '.ecrc', '.flake8', 'grammars', '.devops', 'media', '.editorconfig', 'SECURITY.md', 'LICENSE', 'include', 'requirements', 'flake.lock', 'CMakePresets.json', 'ci', 'build', 'common', '.gitmodules', 'convert_lora_to_gguf.py', 'quantize']\n"
     ]
    }
   ],
   "source": [
    "# ① Point at the real binary in build/bin\n",
    "real_q = os.path.expanduser(\"~/llama.cpp/build/bin/llama-quantize\")\n",
    "assert os.path.exists(real_q), f\"{real_q} not found!\"\n",
    "\n",
    "# ② Make a local 'llama.cpp' folder in your notebook working directory\n",
    "cwd = os.getcwd()\n",
    "local_pack = os.path.join(cwd, \"llama.cpp\")\n",
    "os.makedirs(local_pack, exist_ok=True)\n",
    "\n",
    "# ③ Symlink it as 'llama-quantize' and also as 'quantize'\n",
    "for name in (\"llama-quantize\", \"quantize\"):\n",
    "    link = os.path.join(local_pack, name)\n",
    "    if os.path.exists(link) or os.path.islink(link):\n",
    "        os.remove(link)\n",
    "    os.symlink(real_q, link)\n",
    "\n",
    "# ④ Verify\n",
    "print(\"Notebook sees:\", os.listdir(local_pack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd3a9c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 2.45 out of 15.42 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 102.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Unsloth: Saving tituslhy/llama32_1bn_raft_non_traditional_credentials/pytorch_model.bin...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0', 'q5_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at tituslhy/llama32_1bn_raft_non_traditional_credentials into bf16 GGUF format.\n",
      "The output location will be /home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.BF16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: llama32_1bn_raft_non_traditional_credentials\n",
      "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {2048, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.BF16.gguf: n_tensors = 147, total_size = 2.5G\n",
      "Writing: 100%|██████████| 2.47G/2.47G [00:32<00:00, 76.5Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: /home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
      "main: build = 5361 (cf0a43bb)\n",
      "main: built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n",
      "main: quantizing '/home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.BF16.gguf' to '/home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.Q4_K_M.gguf' as Q4_K_M using 8 threads\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from /home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama32_1Bn_Raft_Non_Traditional_Cred...\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 1.2B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type bf16:  113 tensors\n",
      "[   1/ 147]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   2/ 147]                    rope_freqs.weight - [   32,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 147]                    token_embd.weight - [ 2048, 128256,     1,     1], type =   bf16, converting to q6_K .. size =   501.00 MiB ->   205.49 MiB\n",
      "[   4/ 147]                  blk.0.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[   5/ 147]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   6/ 147]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   7/ 147]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   8/ 147]                  blk.0.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[   9/ 147]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  10/ 147]                blk.0.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  11/ 147]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  12/ 147]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  13/ 147]                  blk.1.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  14/ 147]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  15/ 147]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  16/ 147]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  17/ 147]                  blk.1.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  18/ 147]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  19/ 147]                blk.1.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  20/ 147]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  21/ 147]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  22/ 147]                  blk.2.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  23/ 147]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  24/ 147]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  25/ 147]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  26/ 147]                  blk.2.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  27/ 147]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  28/ 147]                blk.2.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  29/ 147]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  30/ 147]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  31/ 147]                  blk.3.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  32/ 147]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  33/ 147]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  34/ 147]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  35/ 147]                  blk.3.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  36/ 147]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  37/ 147]                blk.3.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  38/ 147]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  39/ 147]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  40/ 147]                  blk.4.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  41/ 147]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  42/ 147]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  43/ 147]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  44/ 147]                  blk.4.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  45/ 147]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  46/ 147]                blk.4.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  47/ 147]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  48/ 147]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  49/ 147]                  blk.5.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  50/ 147]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  51/ 147]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  52/ 147]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  53/ 147]                  blk.5.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  54/ 147]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  55/ 147]                blk.5.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  56/ 147]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  57/ 147]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  58/ 147]                  blk.6.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  59/ 147]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  60/ 147]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  61/ 147]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  62/ 147]                  blk.6.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  63/ 147]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  64/ 147]                blk.6.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  65/ 147]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  66/ 147]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  67/ 147]                  blk.7.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  68/ 147]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  69/ 147]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  70/ 147]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  71/ 147]                  blk.7.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  72/ 147]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  73/ 147]                blk.7.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  74/ 147]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  75/ 147]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  76/ 147]                  blk.8.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  77/ 147]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  78/ 147]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  79/ 147]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  80/ 147]                  blk.8.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  81/ 147]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  82/ 147]                blk.8.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  83/ 147]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  84/ 147]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  85/ 147]                  blk.9.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  86/ 147]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  87/ 147]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  88/ 147]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  89/ 147]                  blk.9.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  90/ 147]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  91/ 147]                blk.9.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  92/ 147]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  93/ 147]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  94/ 147]                 blk.10.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  95/ 147]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  96/ 147]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  97/ 147]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  98/ 147]                 blk.10.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  99/ 147]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 100/ 147]               blk.10.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 101/ 147]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 102/ 147]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 103/ 147]                 blk.11.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 104/ 147]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 105/ 147]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 106/ 147]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 107/ 147]                 blk.11.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 108/ 147]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 109/ 147]               blk.11.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 110/ 147]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 111/ 147]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 112/ 147]                 blk.12.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 113/ 147]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 114/ 147]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 115/ 147]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 116/ 147]                 blk.12.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 117/ 147]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 118/ 147]               blk.12.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 119/ 147]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 120/ 147]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 121/ 147]                 blk.13.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 122/ 147]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 123/ 147]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 124/ 147]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 125/ 147]                 blk.13.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 126/ 147]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 127/ 147]               blk.13.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 128/ 147]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 129/ 147]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 130/ 147]                 blk.14.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 131/ 147]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 132/ 147]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 133/ 147]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 134/ 147]                 blk.14.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 135/ 147]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 136/ 147]               blk.14.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 137/ 147]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 138/ 147]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 139/ 147]                 blk.15.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 140/ 147]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 141/ 147]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 142/ 147]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 143/ 147]                 blk.15.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 144/ 147]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 145/ 147]               blk.15.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 146/ 147]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 147/ 147]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "llama_model_quantize_impl: model size  =  2357.26 MB\n",
      "llama_model_quantize_impl: quant size  =   762.81 MB\n",
      "\n",
      "main: quantize time = 47699.51 ms\n",
      "main:    total time = 47699.51 ms\n",
      "Unsloth: Conversion completed! Output location: /home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.Q4_K_M.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q8_0. This might take 20 minutes...\n",
      "main: build = 5361 (cf0a43bb)\n",
      "main: built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n",
      "main: quantizing '/home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.BF16.gguf' to '/home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.Q8_0.gguf' as Q8_0 using 8 threads\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from /home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama32_1Bn_Raft_Non_Traditional_Cred...\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 1.2B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type bf16:  113 tensors\n",
      "[   1/ 147]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   2/ 147]                    rope_freqs.weight - [   32,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 147]                    token_embd.weight - [ 2048, 128256,     1,     1], type =   bf16, converting to q8_0 .. size =   501.00 MiB ->   266.16 MiB\n",
      "[   4/ 147]                  blk.0.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[   5/ 147]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   6/ 147]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[   7/ 147]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[   8/ 147]                  blk.0.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[   9/ 147]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  10/ 147]                blk.0.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  11/ 147]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  12/ 147]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  13/ 147]                  blk.1.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  14/ 147]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  15/ 147]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  16/ 147]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  17/ 147]                  blk.1.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  18/ 147]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  19/ 147]                blk.1.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  20/ 147]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  21/ 147]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  22/ 147]                  blk.2.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  23/ 147]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  24/ 147]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  25/ 147]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  26/ 147]                  blk.2.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  27/ 147]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  28/ 147]                blk.2.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  29/ 147]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  30/ 147]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  31/ 147]                  blk.3.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  32/ 147]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  33/ 147]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  34/ 147]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  35/ 147]                  blk.3.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  36/ 147]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  37/ 147]                blk.3.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  38/ 147]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  39/ 147]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  40/ 147]                  blk.4.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  41/ 147]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  42/ 147]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  43/ 147]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  44/ 147]                  blk.4.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  45/ 147]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  46/ 147]                blk.4.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  47/ 147]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  48/ 147]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  49/ 147]                  blk.5.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  50/ 147]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  51/ 147]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  52/ 147]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  53/ 147]                  blk.5.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  54/ 147]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  55/ 147]                blk.5.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  56/ 147]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  57/ 147]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  58/ 147]                  blk.6.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  59/ 147]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  60/ 147]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  61/ 147]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  62/ 147]                  blk.6.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  63/ 147]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  64/ 147]                blk.6.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  65/ 147]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  66/ 147]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  67/ 147]                  blk.7.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  68/ 147]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  69/ 147]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  70/ 147]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  71/ 147]                  blk.7.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  72/ 147]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  73/ 147]                blk.7.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  74/ 147]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  75/ 147]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  76/ 147]                  blk.8.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  77/ 147]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  78/ 147]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  79/ 147]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  80/ 147]                  blk.8.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  81/ 147]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  82/ 147]                blk.8.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  83/ 147]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  84/ 147]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  85/ 147]                  blk.9.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  86/ 147]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  87/ 147]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  88/ 147]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  89/ 147]                  blk.9.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  90/ 147]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  91/ 147]                blk.9.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  92/ 147]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  93/ 147]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  94/ 147]                 blk.10.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  95/ 147]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  96/ 147]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  97/ 147]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  98/ 147]                 blk.10.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[  99/ 147]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 100/ 147]               blk.10.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 101/ 147]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 102/ 147]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 103/ 147]                 blk.11.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[ 104/ 147]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 105/ 147]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 106/ 147]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 107/ 147]                 blk.11.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[ 108/ 147]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 109/ 147]               blk.11.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 110/ 147]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 111/ 147]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 112/ 147]                 blk.12.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[ 113/ 147]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 114/ 147]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 115/ 147]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 116/ 147]                 blk.12.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[ 117/ 147]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 118/ 147]               blk.12.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 119/ 147]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 120/ 147]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 121/ 147]                 blk.13.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[ 122/ 147]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 123/ 147]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 124/ 147]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 125/ 147]                 blk.13.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[ 126/ 147]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 127/ 147]               blk.13.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 128/ 147]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 129/ 147]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 130/ 147]                 blk.14.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[ 131/ 147]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 132/ 147]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 133/ 147]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 134/ 147]                 blk.14.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[ 135/ 147]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 136/ 147]               blk.14.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 137/ 147]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 138/ 147]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 139/ 147]                 blk.15.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[ 140/ 147]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 141/ 147]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 142/ 147]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 143/ 147]                 blk.15.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\n",
      "[ 144/ 147]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 145/ 147]               blk.15.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 146/ 147]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 147/ 147]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "llama_model_quantize_impl: model size  =  2357.26 MB\n",
      "llama_model_quantize_impl: quant size  =  1252.41 MB\n",
      "\n",
      "main: quantize time = 19925.19 ms\n",
      "main:    total time = 19925.19 ms\n",
      "Unsloth: Conversion completed! Output location: /home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.Q8_0.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This might take 20 minutes...\n",
      "main: build = 5361 (cf0a43bb)\n",
      "main: built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n",
      "main: quantizing '/home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.BF16.gguf' to '/home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.Q5_K_M.gguf' as Q5_K_M using 8 threads\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from /home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama32_1Bn_Raft_Non_Traditional_Cred...\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 1.2B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type bf16:  113 tensors\n",
      "[   1/ 147]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   2/ 147]                    rope_freqs.weight - [   32,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 147]                    token_embd.weight - [ 2048, 128256,     1,     1], type =   bf16, converting to q6_K .. size =   501.00 MiB ->   205.49 MiB\n",
      "[   4/ 147]                  blk.0.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[   5/ 147]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   6/ 147]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[   7/ 147]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[   8/ 147]                  blk.0.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[   9/ 147]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  10/ 147]                blk.0.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  11/ 147]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  12/ 147]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  13/ 147]                  blk.1.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  14/ 147]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  15/ 147]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  16/ 147]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  17/ 147]                  blk.1.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  18/ 147]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  19/ 147]                blk.1.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  20/ 147]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  21/ 147]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  22/ 147]                  blk.2.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  23/ 147]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  24/ 147]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  25/ 147]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  26/ 147]                  blk.2.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  27/ 147]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  28/ 147]                blk.2.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  29/ 147]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  30/ 147]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  31/ 147]                  blk.3.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  32/ 147]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  33/ 147]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  34/ 147]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  35/ 147]                  blk.3.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  36/ 147]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  37/ 147]                blk.3.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  38/ 147]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  39/ 147]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  40/ 147]                  blk.4.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  41/ 147]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  42/ 147]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  43/ 147]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  44/ 147]                  blk.4.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  45/ 147]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  46/ 147]                blk.4.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  47/ 147]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  48/ 147]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  49/ 147]                  blk.5.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  50/ 147]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  51/ 147]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  52/ 147]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  53/ 147]                  blk.5.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  54/ 147]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  55/ 147]                blk.5.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  56/ 147]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  57/ 147]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  58/ 147]                  blk.6.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  59/ 147]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  60/ 147]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  61/ 147]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  62/ 147]                  blk.6.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  63/ 147]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  64/ 147]                blk.6.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  65/ 147]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  66/ 147]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  67/ 147]                  blk.7.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  68/ 147]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  69/ 147]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  70/ 147]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  71/ 147]                  blk.7.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  72/ 147]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  73/ 147]                blk.7.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  74/ 147]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  75/ 147]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  76/ 147]                  blk.8.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  77/ 147]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  78/ 147]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  79/ 147]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  80/ 147]                  blk.8.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  81/ 147]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  82/ 147]                blk.8.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  83/ 147]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  84/ 147]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  85/ 147]                  blk.9.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  86/ 147]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  87/ 147]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  88/ 147]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  89/ 147]                  blk.9.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  90/ 147]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  91/ 147]                blk.9.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  92/ 147]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  93/ 147]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  94/ 147]                 blk.10.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[  95/ 147]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  96/ 147]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  97/ 147]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  98/ 147]                 blk.10.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  99/ 147]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 100/ 147]               blk.10.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 101/ 147]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 102/ 147]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 103/ 147]                 blk.11.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[ 104/ 147]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 105/ 147]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 106/ 147]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 107/ 147]                 blk.11.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[ 108/ 147]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 109/ 147]               blk.11.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 110/ 147]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 111/ 147]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 112/ 147]                 blk.12.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[ 113/ 147]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 114/ 147]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 115/ 147]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 116/ 147]                 blk.12.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[ 117/ 147]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 118/ 147]               blk.12.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 119/ 147]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 120/ 147]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 121/ 147]                 blk.13.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[ 122/ 147]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 123/ 147]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 124/ 147]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 125/ 147]                 blk.13.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 126/ 147]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 127/ 147]               blk.13.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 128/ 147]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 129/ 147]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 130/ 147]                 blk.14.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[ 131/ 147]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 132/ 147]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 133/ 147]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 134/ 147]                 blk.14.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 135/ 147]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 136/ 147]               blk.14.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 137/ 147]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 138/ 147]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 139/ 147]                 blk.15.attn_k.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q5_K .. size =     2.00 MiB ->     0.69 MiB\n",
      "[ 140/ 147]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 141/ 147]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 142/ 147]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 143/ 147]                 blk.15.attn_v.weight - [ 2048,   512,     1,     1], type =   bf16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 144/ 147]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 145/ 147]               blk.15.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 146/ 147]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 147/ 147]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "llama_model_quantize_impl: model size  =  2357.26 MB\n",
      "llama_model_quantize_impl: quant size  =   861.81 MB\n",
      "\n",
      "main: quantize time = 34068.61 ms\n",
      "main:    total time = 34068.61 ms\n",
      "Unsloth: Conversion completed! Output location: /home/ubuntu/ideal-palm-tree/notebooks/tituslhy/llama32_1bn_raft_non_traditional_credentials/unsloth.Q5_K_M.gguf\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a2eebeda35458eac351b1f555ab422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q4_K_M.gguf:   0%|          | 0.00/808M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/tituslhy/llama32_1bn_raft_non_traditional_credentials\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a648d76d0a04e818e5c92106c041701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q8_0.gguf:   0%|          | 0.00/1.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/tituslhy/llama32_1bn_raft_non_traditional_credentials\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ac921efd2c419d90025ade60033f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q5_K_M.gguf:   0%|          | 0.00/912M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/tituslhy/llama32_1bn_raft_non_traditional_credentials\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub_gguf(\n",
    "    \"tituslhy/llama32_1bn_raft_non_traditional_credentials\", # Change hf to your username!\n",
    "    tokenizer,\n",
    "    quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "    token = os.environ[\"HUGGINGFACE_ACCESS_TOKEN\"], # Get a token at https://huggingface.co/settings/tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928b9e2",
   "metadata": {},
   "source": [
    "Test to see if we can pull our model using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull hf.co/tituslhy/llama32_1bn_raft_non_traditional_credentials:Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b84cbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='Hi! How can I help you today? Do you have a question or need assistance with something.', additional_kwargs={'tool_calls': []}, raw={'model': 'hf.co/tituslhy/llama32_1bn_raft_non_traditional_credentials:Q4_K_M', 'created_at': '2025-05-20T06:47:35.098009168Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2904112368, 'load_duration': 2137232750, 'prompt_eval_count': 12, 'prompt_eval_duration': 644078380, 'eval_count': 21, 'eval_duration': 122027466, 'message': Message(role='assistant', content='Hi! How can I help you today? Do you have a question or need assistance with something.', images=None, tool_calls=None), 'usage': {'prompt_tokens': 12, 'completion_tokens': 21, 'total_tokens': 33}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    \"hf.co/tituslhy/llama32_1bn_raft_non_traditional_credentials:Q4_K_M\"\n",
    ")\n",
    "\n",
    "llm.complete(\"Hi!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
